\documentclass[12pt,halfline,a4paper]{ouparticle}

\begin{document}

\title{Using a Two-Layer Neural Network and Physicochemical Properties to Classify Glass for Forensic Analysis}

\author{%
\name{Michael Arango}
\email{mikearango@gwu.edu}
	\and
\name{Mark Barna}
\email{mark.barna@gmail.com}
	\and
\name{Paul Brewster}
\email{pfbrewster@gmail.com}
}

%\abstract{abstract text goes here...}
\date{\today}
\maketitle
\pagebreak
\tableofcontents
\pagebreak

\section{Project Proposal}
\label{sec1}
In this study, we will use the physicochemical properties of glass to determine whether or not a given glass sample was taken from a window. This is a fundamental problem in forensic analysis as it is highly unlikely that glass fragments will be found on people unless they have been present at the time glass breaks. Glass analysis is of vital importance in forensic science as it allows us to test if the glass fragment found on a person is the same as the glass at a crime scene. Since glass is made up of several raw materials and certain elements impart specific properties, we can find out a lot about the glass if we analyze the chemical composition.

The dataset we chose for analysis was made available for download from the UCI Machine Learning Repository and was created by the USA Forensic Science Service. There are 214 observations of 9 different features along with 214 targets that specify whether the glass sample came from a window or not. While we would like more data to train a neural network, we believe the dataset is large enough for our purposes. It is difficult to know before we train a neural network if we have enough data, but the amount of data required is directly related to the complexity of the underlying decision boundary we are trying to implement. We won't 
know how complex the decision boundary we are trying to approximate is until we train the network, but we feel confident using the dataset as many others have used the dataset and found robust results. Several other papers in the literature use much more complex methods than we will employ and have not found the size of the data to be an issue. 

We have chosen a two-layer perceptron network with tangent-sigmoid transfer functions in the hidden layer and \emph{softmax} transfer functions in the output layer. This is a fairly standard network for pattern recognition. Moreover, we will use the \emph{Scaled Conjugate Gradient (SGD)} algorithm to train the network as it is good for pattern recognition problems in which the output layer uses a non-linear transfer function. Since we do not expect the training error to converge to zero, we implement early stopping criteria to prevent overfitting. Lastly, we use \emph{cross-entropy} as our performance index since our targets take on discrete values and it is the optimal performance index for pattern recognition networks that use the \emph{softmax} transfer function in the output layer. 

Two different frameworks will be used to implement the neural network. First, we will use the Neural Network Toolbox, specifically the Neural Network Pattern Recognition Tool (\verb|nprtool|) train, validate, and test our network. We use this framework to start with a simple graphical user interface to quickly ensure our specified network architecture is appropriate and to get baseline performance statistics. Then, we will replicate the analysis in Python to gain practical experience building network architectures in a scripting language. Note that since the goal is practical experience, we will not be leveraging the power of the \emph{scikit-learn} package (\verb|sklearn|) for this exercise. 

Several reference materials will be consulted to obtain sufficient background knowledge of the subject at hand. First, we plan on doing a thorough review of the forensic chemistry and geology literature to understand the reasons for using physicochemical properties to classify glass. Then, papers on glass analysis will be examined to supplement background knowledge with experiential knowledge. 

Considering our problem is one of pattern recognition, a confusion matrix will be used to assess the accuracy of our model and the \emph{false postive} (Type I error) and \emph{false negative} (Type II error) rates. Further, the \emph{Receiver Operating Characteristic (ROC) curve} will be used to compare the true positive rate to the false positive rate. This will help us gain additional knowledge of the predictive power of our network. 

We plan to finish our research and submit it by Wednesday, June 28, 2017. 

\section{Introduction}
\label{sec2}
An overview of the project and an outline of the report (I like to write intro after I finish a project).

\subsection{Literature Review}
\label{sec3}

\section{Description of the Dataset}
\subsection{Inputs}
\subsection{Targets}

\section{Description of the Network Architecture and Training Algorithm}

\subsection{Network Architecture}
\subsection{Training Algorith}

\section{Experimental Setup}

\subsection{Data Preprocessing}
\subsection{Implementation of the Network}
\subsection{Performance Index}

\section{Results}

\section{Conclusion}

\subsection{General guidelines}
\label{sec3.1}

By default, all of the options within \verb+article.cls+ are available
with this class file. This class file provides the following additional options.

\begin{description}

\item \textbf{endnotes:}
To make all footnotes to endnotes. You may follow the same
coding \verb+\footnote{text}+ for both footnotes and endnotes. Once you use this option
you have to use the \verb+\theendnotes+ command at the place where all the endnotes
have to be set in your paper.

\item \textbf{numbib:}
This is the default option that numbers the bibliography items;
this option does nothing with natbib and other packages.

\end{description}

\subsection{Sections and subsections}
\label{sec3.6}

 If you want to be able to make reference to that section,
then you need to \texttt{label} it (see Section \ref{sec3.14}). 

\subsubsection*{\LaTeX\ text formatting commands}
\begin{tabular}{ll@{\hskip60pt}ll}
\verb+\textit+  & Italics      &\verb+\textsf+  & Sans Serif\\
\verb+\textbf+  & Boldface     &\verb+\textsc+  & Small Caps\\
\verb+\texttt+  & Typewriter   &\verb+\textmd+  & Medium Series\\
\verb+\textrm+  & Roman        &\verb+\textnormal+ & Normal Series\\
\verb+\textsl+  & Slanted      &\verb+\textup+  & Upright Series
\end{tabular}

\subsubsection*{\LaTeX\ math formatting commands}
\begin{tabular}{ll@{\qquad}ll}
\verb+\mathit+     & Math Italics            &\verb+\mathfrak+   & Fraktur\\
\verb+\mathbf+     & Math Boldface       &\verb+\mathbb+     & Blackboard Bold\\
\verb+\mathtt+     & Math Typewriter     &\verb+\mathnormal+ & Math Normal\\
\verb+\mathsf+     & Math Sans Serif     &\verb+\boldsymbol+ & Bold math for Greek letters\\
\verb+\mathcal+    & Calligraphic        &                   & and other symbols
\end{tabular}


\subsection{Figures and tables}
\label{sec3.9}

The following is an example of typesetting a table.

\begin{verbatim}
\begin{table}
\caption{Table caption text.}
\label{key}
The table matter goes here.
\end{table}
\end{verbatim}

As always with \LaTeX, the \verb+\label+ must be after the
\verb+\caption+, and inside the figure or table environment. The reference for
figures and tables inside text can be made using the \verb|\ref{key}| command.

\subsection{Equations}\label{sec3.10}

For example, if you type
\begin{verbatim}
\begin{equation}\label{eq1}
\int^{r_2}_0 F(r,\varphi){\rm d}r\,{\rm d}\varphi = [\sigma r_2/(2\mu_0)]
\int^{\infty}_0\exp(-\lambda|z_j-z_i|)\lambda^{-1}J_1 (\lambda r_2)J_0
(\lambda r_i\,\lambda {\rm d}\lambda)
\end{equation}
\end{verbatim}
then you will get the following output:
\begin{equation}\label{eq1}
\int^{r_2}_0 F(r,\varphi){\rm d}r\,{\rm d}\varphi = [\sigma r_2/(2\mu_0)]\int^{\infty}_0
\exp(-\lambda|z_j-z_i|)\lambda^{-1}J_1 (\lambda r_2)J_0 (\lambda r_i\,\lambda {\rm d}\lambda)
\end{equation}

\subsection{Listings}
\label{sec3.12}

Another frequently displayed structure is a list. The
following is an example of an \emph{itemized} list.
\begin{itemize}
   \item This is the first item of an itemized list.
         Each item in the list is marked with a
         `$\bullet$'.

   \item This is the second item of the list. It
         contains another list nested inside it. The
         inner list is an \emph{enumerated} list.
         \begin{enumerate}
            \item This is the first item of an enumerated
                  list that is nested within the
                  itemized list.

            \item This is the second item of the inner list.
                  \LaTeX\ allows you to nest lists
                  deeper than you really should.
         \end{enumerate}
         This is the rest of the second item of the
         outer list. It is no more interesting than
         any other part of the item.
   \item This is the third item of the list.
\end{itemize}


\pagebreak

\addcontentsline{toc}{section}{References}
\begin{thebibliography}{0}
\bibitem{bib1}
Goossens, M., F. Mittelbach, and A. Samarin: {\em The {\LaTeX} Companion}.
Addison-Wesley, Reading, MA, USA, 1994.
\bibitem{bib2}
Knuth, D.E: {\em The {\TeX}book}. Addison-Wesley, Reading, MA, USA, 1984.
\bibitem{bib3}
Lamport, L.: {\em {\LaTeX} -- A Document Preparation System -- User's
Guide and Reference Manual}. Addison-Wesley, Reading, MA, USA, 1985.
\bibitem{bib4}
Smith, I.N., R.S. Johnes, and W.P. Hines: 1992, `Title of the Article',
\textit{Journal Title in Italics} \textbf{Vol. no. X}, pp. 00--00
\end{thebibliography}

\pagebreak
\appendix

\section{Perceptron}
\section{Backprop}
\section{Subroutine X}


\end{document}
