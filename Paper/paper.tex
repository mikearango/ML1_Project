\documentclass[12pt,halfline,a4paper]{ouparticle}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{tikz}
\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{algpseudocode}
\usetikzlibrary{automata,positioning}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\usepackage[nottoc]{tocbibind}
\addbibresource{paper.bib}

% defining colors chunks 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% define a global listing style
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=3pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=1
}
 
% set global listing style
\lstset{style=mystyle}

% define styles for each language

\lstdefinestyle{R}
    {language = R, style = mystyle}
\lstdefinestyle{Python}
    {language = Python, style = mystyle}
\lstdefinestyle{Matlab}
    {language = Matlab, style = mystyle}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% title page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Using a Two-Layer Neural Network and Physicochemical Properties to Classify Glass for Forensic Analysis}

\author{%
\name{Michael Arango}
\email{mikearango@gwu.edu}
	\and
\name{Mark Barna}
\email{mark.barna@gmail.com}
	\and
\name{Paul Brewster}
\email{pfbrewster@gmail.com}
}

%\abstract{abstract text goes here...}
\date{\today}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% table of contents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagebreak
\tableofcontents
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% paper
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Project Proposal}
\label{sec1}
In this study, we will use the physicochemical properties of glass to determine whether or not a given glass sample was taken from a window. This is a fundamental problem in forensic analysis as it is highly unlikely that glass fragments will be found on people unless they have been present at the time glass breaks. Glass analysis is of vital importance in forensic science as it allows us to test if the glass fragment found on a person is the same as the glass at a crime scene. Since glass is made up of several raw materials and certain elements impart specific properties, we can find out a lot about the glass if we analyze the chemical composition.

The dataset we chose for analysis was made available for download from the UCI Machine Learning Repository and was created by the USA Forensic Science Service. There are 214 observations of 9 different features along with 214 targets that specify whether the glass sample came from a window or not. While we would like more data to train a neural network, we believe the dataset is large enough for our purposes. It is difficult to know before we train a neural network if we have enough data, but the amount of data required is directly related to the complexity of the underlying decision boundary we are trying to implement. We won't 
know how complex the decision boundary we are trying to approximate is until we train the network, but we feel confident using the dataset as many others have used the dataset and found robust results. Several other papers in the literature use much more complex methods than we will employ and have not found the size of the data to be an issue. 

We have chosen a two-layer perceptron network with tangent-sigmoid transfer functions in the hidden layer and \emph{softmax} transfer functions in the output layer. This is a fairly standard network for pattern recognition. Moreover, we will use the \emph{Scaled Conjugate Gradient (SGD)} algorithm to train the network as it is good for pattern recognition problems in which the output layer uses a non-linear transfer function. Since we do not expect the training error to converge to zero, we implement early stopping criteria to prevent overfitting. Lastly, we use \emph{cross-entropy} as our performance index since our targets take on discrete values and it is the optimal performance index for pattern recognition networks that use the \emph{softmax} transfer function in the output layer. 

Two different frameworks will be used to implement the neural network. First, we will use the Neural Network Toolbox, specifically the Neural Network Pattern Recognition Tool (\verb|nprtool|) train, validate, and test our network. We use this framework to start with a simple graphical user interface to quickly ensure our specified network architecture is appropriate and to get baseline performance statistics. Then, we will replicate the analysis in Python to gain practical experience building network architectures in a scripting language. Note that since the goal is practical experience, we will not be leveraging the power of the \emph{scikit-learn} package (\verb|sklearn|) for this exercise. 

Several reference materials will be consulted to obtain sufficient background knowledge of the subject at hand. First, we plan on doing a thorough review of the forensic chemistry and geology literature to understand the reasons for using physicochemical properties to classify glass. Then, papers on glass analysis will be examined to supplement background knowledge with experiential knowledge. 

Considering our problem is one of pattern recognition, a confusion matrix will be used to assess the accuracy of our model and the \emph{false postive} (Type I error) and \emph{false negative} (Type II error) rates. Further, the \emph{Receiver Operating Characteristic (ROC) curve} will be used to compare the true positive rate to the false positive rate. This will help us gain additional knowledge of the predictive power of our network. 

We plan to finish our research and submit it by Wednesday, June 28, 2017. 

\section{Literature Review}
\label{sec2}
The dataset we will use for this analysis was initially employed in Evett and Spiehler's paper \emph{Rule Induction in Forensic Science} (1987) \cite{evett1987}. They recognized the usefulness to a forensic crime lab of classifying glass fragments based on refractive index and chemical composition. This would, for example, allow the lab to ascertain whether samples gathered on a suspect?s clothing came from a window, potentially indicating they had broken it, or from another source, like a broken bottle. 

In their experiment, Evett and Spiehler wished to see if the Bionic Evolutionary Algorithm Generating Logical Expressions (BEAGLE) machine learning algorithm could correctly classify the glass---first as either window or non-window, and then into a second level of sub-categories. Our project focuses on the former and leaves the latter as a future exercise. The BEAGLE algorithm Evett and Spiehler used to train the network uses a series of logical ``and'' statements to chain rules together based on the inputs. They offer up the following example of a rule: 
\begin{equation}
\{(\mathrm{Fe} \leq \mathrm{Na}) \text{ and } [\mathrm{K} > (\mathrm{Fe} \cdot 650)]\}, 
\end{equation}
where $\mathrm{Fe}$, $\mathrm{Na}$, and $\mathrm{K}$ are the percent composition of iron, sodium, and potassium, respectively, of each glass sample. Evett and Spiehler found that the BEAGLE algorithm outperformed the $k$-Nearest Neighbors algorithm (with $ k = 3$) and Linear Discriminant Analysis (LDA)---the models they used for baseline performance measures. 

The Department of Justice regularly issues research grants for the elemental analysis of glass. In 2012, they issued one such grant to researchers at Florida International University (FIU) to work with the Miami-Dade County Police Department \cite{almirall2012}. The researchers note that analysis of small quantities of materials has become an important yet underutilized type of evidence at many crime scenes including hit-and-run accidents and other violent crimes. The ability to classify different types of glass could be of vital importance in the case of a hit-and-run. Further, the group of researchers attempted to compare the discrimination power between the methods used in most forensic laboratories for glass analysis. Their aim was to create a more ``standard'' method that can be used by the operational forensic laboratory and a ``match criteria'' for use in routine casework situations.

Maureen Bottrell, a geologist and forensic scientist at the FBI Laboratory released a report in 2009 documenting the background information that ought to be used when comparing glass samples with data \cite{bottrell2009}. She notes that the vast majority of raw materials used to make glass are derived geologically and that North American glass makers use more than 20 million tons of raw materials annually. All of these materials contain several impurities that result in perceived differences in glass products. 

Bottrell writes that physical properties such as color, curvature, fluorescence, thickness, and surface features should first be used to determine if the material fragments are glass. Once we know a sample is glass, Bottrell recommends using optical properties, particle immersion, density, and elemental analysis to differentiate between types of glass. In this study, we focus on optical properties, specifically the refractive index, and elemental analysis to classify whether glass samples came from a window or not.\footnote{See section \ref{sec3.1} for more on the refractive index.}

Since glass is made up of several raw materials and certain elements impart specific properties, we can find out a lot about the glass if we analyze the chemical composition. Glass made on the same manufacturing line over a period of time can often have highly variable properties as mixtures of raw materials can have drastically different chemical compositions. 

\section{Description of the Dataset}
\label{sec3}
The dataset was made available for download from the UCI Machine Learning Repository and was created by the USA Forensic Science Service \cite{murphy1994}. The purpose of this dataset is to use physicochemical properties to classify whether a certain glass fragment comes from a window or not. 

\subsection{Inputs}
\label{sec3.1}
The matrix of inputs contains 214 observations of 9 variables and there is no missing data. Of these variables, eight of the nine measure the percent weight that a given elemental oxide makes up of the total glass sample weight. All the eight elements except silicon are classified as metals on the periodic table of elements. Sodium and potassium are alkali metals whereas magnesium, calcium, and barium alkaline earth metals. Aluminum and iron are classified as poor metals and transition metals, respectively. The last variable in the input matrix represents the refractive index which measures the speed of light in a transparent medium and is known as Snell's law. It can be represented formulaically as the ratio of the velocity of light in a vacuum to the velocity of light in the glass itself: $n = \frac{c}{v}$. A more thorough description of target variables is as follows: 
\begin{description}
\item \textbf{Refractive Index:}
measures the ratio of the velocity of light in a vacuum to the velocity of light in the glass itself
\item \textbf{Sodium:}
represents the percent weight in sodium oxide ($\mathrm{Na_{2}O}$)
\item \textbf{Magnesium:}
represents the percent weight in magnesium oxide ($\mathrm{MgO}$)
\item \textbf{Aluminum:}
represents the percent weight in aluminum oxide ($\mathrm{Na_{2}O}$)
\item \textbf{Silicon:}
represents the percent weight in silicon oxide ($\mathrm{Al_{2}O_{3}}$)
\item \textbf{Potassium:}
represents the percent weight in potassium oxide ($\mathrm{K_{2}O}$)
\item \textbf{Calcium:}
represents the percent weight in calcium oxide ($\mathrm{Ca_{2}O}$)
\item \textbf{Barium:}
represents the percent weight in barium oxide ($\mathrm{Ba_{2}O}$)
\item \textbf{Iron:}
represents the percent weight in iron oxide ($\mathrm{Fe_{2}O_{3}}$)
\end{description}

\subsection{Targets}
\label{sec3.2}
The matrix of targets has 214 observations, one for each observation in the training set, where a given target is denoted by $\begin{bmatrix} 1 \\ 0 \end{bmatrix}$ if the glass sample comes from a window and $\begin{bmatrix} 0 \\ 1 \end{bmatrix}$ otherwise. Note that the targets are two-dimensional instead of the more common one-dimensional binary encoding. This two-dimensional encoding allows us to have only one neuron firing at a time and tends to result in marginally better performance. 

\section{Description of the Network Architecture and Training Algorithm}
\label{sec4}
Once the data is preprocessed, the next step is to decide on or create a network architecture. The basic network architecture is determined by the problem we wish to solve. Once the basic network architecture is determined, we decide how many layers, how many neurons in each layer, how many outputs the network should have, and what kind of performance index function we should use for training \cite{hagan2014}. 

\subsection{Network Architecture}
\label{sec4.1}
The standard neural network architecture for pattern recognition problems is the multi-layer perceptron with tangent-sigmoid transfer functions in the hidden layers and \emph{softmax} transfer functions in the output layer. For most problems, including a fairly simple one like ours, one hidden layer usually suffices. Thus, we will implement a two-layer perceptron. If the results of the network are unsatisfactory after training and testing with one hidden layer, we will retrain with an additional hidden layer, but we do not anticipate having to do this. The \emph{tansig} transfer function is usually preferred to the \emph{tansig} transfer function in the hidden layers since it produces outputs (which are inputs to the next layer) that are centered near zero, whereas the \emph{tansig} transfer function always produces positive outputs. 

We also need to select the number of neurons in each layer. The number of neurons we use in the output layer should be the same as the size of the target vector. In our case, this means we should use two neurons in the output layer. On the other hand, the number of neurons we use in the hidden layer is directly proportional to the the complexity of the decision boundary being implemented. Since we do not know the complexity of the decision boundary needed to classify these glass samples before training, we begin with ten neurons, which may be more than we need, and leverage early stopping techniques to prevent overfitting \cite{hagan2014}. 

Now that we have chosen a network architecture, we can calculate the network output. The output from the hidden layer (the input to the output layer) can be calculated as 
\begin{equation}
\mathbf{a^{1}} = \textbf{tansig}(\mathbf{W^{1}}\mathbf{p} + \mathbf{b^{1}}), 
\end{equation}
while the output from the output layer is 
\begin{equation}
\mathbf{a^{2}} = \textbf{softmax}(\mathbf{W^{2}}\mathbf{a^{1}} + \mathbf{b^{2}}). 
\end{equation}
The network architecture can be seen in \textbf{Figure 1} below. 
\begin{figure}[H]
\includegraphics[width = 6in]{figs/network.png}
\caption{Two-Layer Perceptron Used to Classify Glass Samples}
\end{figure}

\subsubsection{History of the Perceptron}
\label{sec4.1.1}
Before discussing the training algorithm, we offer up a brief history of the perceptron. Much of the modern interpretation of neural networks is credited in large part to Warren McCulloch and Walter Pitts who showed that networks of artificial neurons could calculate any function. We still use the fundamental feature of their model in which a weighted sum of inputs is compared to a threshold to determine the output of a neuron \cite{hagan2014}.

One of the first applications of neural networks came in the 1950's when Frank Rosenblatt developed the perceptron network and the corresponding learning rule to solve pattern recognition problems. While his developments were monumental at the time, several researchers showed that a single-layer perceptron and the learning rule could not solve certain problems. Specifically, the error will never converge to zero when using the perceptron learning rule if the input vectors are \emph{linearly inseparable}. It wasn't until the 1980's that multi-layer perceptron networks and more complex learning rules were proposed that could solve these problems. 

Widrow and Hoff's Least Mean Square (LMS) learning rule suffered from the same disadvantage as Rosenblatt's, but it has since been generalized. The generalization of the LMS learning rule is referred to as \emph{backpropagation} and we commonly use it to train multi-layer perceptron networks \cite{hagan2014}. 

\subsection{Training Algorithm}
\label{sec4.2}
We chose to use the Scaled Conjugate Gradient (SCG) algorithm in MATLAB to train our network as it is very efficient for pattern recognition problems. For multi-layer networks, the Levenberg-Marquardt algorithm is often used, but it does not work well for pattern recognition as the transfer function in the output layer is operating outside the linear region. The scaled conjugate gradient algorithm is a special type of backpropagation. For the replication of the network in Python, we use standard backpropagation with steepest descent. 

The implementation of backpropagation can be broken down into three steps: 
\begin{enumerate}
\item Propagate the input forward through the network
\item Propagate the sensitivities backward through the network
\item Update the weights and biases using the approximate steepest descent rule
\end{enumerate}

\subsubsection{Forward Propagation}
\label{sec4.2.1}
For a multilayer network, the output from one layer is the input to the next layer. That is, the hidden layer's output is the input to the output layer of the network. The neurons in the first layer receive external inputs (the observed data):
\begin{equation}
\mathbf{a}^{0} = \mathbf{p},
\end{equation}
which serves as the starting point for the network. After the input is received, it is propagated forward with the following equation
\begin{equation}
\mathbf{a}^{m+1} = \mathbf{f}^{m+1}(\mathbf{W}^{m+1}\mathbf{a}^{m} + \mathbf{b}^{m+1}), \text{ for \emph{m} = 0, 1, \ldots, $M - 1$} 
\end{equation}
where we use $M$ to denote the number of layers in the network. Then, the output from the the neurons in the last layer are
\begin{equation}
\mathbf{a} = \mathbf{a}^{M}.
\end{equation}

\subsubsection{Performance Index}
\label{sec4.2.2}
Before moving on to the step of backpropagating the sensitivities, we discuss the performance index as it is a critical part of the process. The performance index is a measure of the error of the network outputs in relation to the targets. The general implementation of the backpropagation algorithm uses the \emph{mean square error} as a performance index. The mean square error is approximated by taking the expectation of the sum of the squared errors (residuals). However, we choose a different performance index to gauge our model's predictions. 

Mean square error works very well for functions with continuous target values---the case where we are approximating a function. However, in pattern recognition we are given discrete target values, so other performance indices that take this into account make more sense. We choose to use \emph{cross-entropy} as our performance index. This is a commonly used performance index when the \emph{softmax} transfer function is used in the output layer. Given a set of input-target pairs 
\begin{equation}
\{\mathbf{p}_{1}, \mathbf{t}_{1}\}, \{\mathbf{p}_{2}, \mathbf{t}_{2}\}, \ldots, \{\mathbf{p}_{Q}, \mathbf{t}_{Q}\}
\end{equation}
where $\mathbf{p}_{q}$ is an input and $\mathbf{t}_{q}$ is the corresponding target output, we denote the cross-entropy loss as follows: 
\begin{equation}
F(\mathbf{x}) = - \sum_{q = 1}^{Q} \sum_{i = 1}^{S^{M}} t_{i, q} \ln \frac{a_{i, q}}{t_{i, q}}
\end{equation}
where $Q$ is the number of samples in the dataset and $S^{M}$ is the number of neurons in the output layer. We can simplify this by vectorizing the operation over all input-target pairs, $\{\mathbf{p}_{q}, \mathbf{t}_{q}\}$, and eliminating the first summation: 
\begin{equation}
- \sum_{i = 1}^{S^{M}} \mathbf{t}_{i} \ln \frac{\mathbf{a}_{i}}{\mathbf{t}_{i}}.
\end{equation}
Recall that in our pattern recognition problem we have two classes where the targets are $\begin{bmatrix} 1 \\ 0 \end{bmatrix}$ and $\begin{bmatrix} 0 \\ 1 \end{bmatrix}$. Thus, each neuron can only take on values of zero or one. This allows us to further simplify the equation to 
\begin{equation}
- \mathbf{t} \ln \mathbf{a}
\end{equation}
since $\mathbf{t}_{i} = 0$ implies the cross-entropy loss for the $i$-th neuron is zero and the case where 
$\mathbf{t}_{i} = 1$ implies the cross-entropy loss is simply $ - \mathbf{t}_{i} \ln \mathbf{a}_{i}$.  

It is important to note the backpropagation algorithm works with any differentiable performance index we specify. We just need to change the initialization of the sensitivities in the output layer accordingly. This brings us back to the next step: propagating the sensitivities backward through the network \cite{hagan2014}.

\subsubsection{Backpropagation}
\label{sec4.2.3}
Once we have propagated the input forward, we tcompare the network output to the targets so we can use the error to adjust the weights and biases accordingly. But, in the case of a multi-layer network, the errors and the performance index we use to evaluate those errors are no longer just a function of the weights. Rather, they are indirectly a function of the weights in the hidden layer in addition to the weights in the output layer. Thus, we call the errors we propagate backward through the network `sensitivities' because they represent the sensitivity of the performance index, $F(\mathbf{x})$, to changes in the $i$-th element of the net input at layer $m$. We can calculate these sensitivities analytically as follows: 
\begin{equation}
s^{m}_{i} \equiv \frac{\partial F}{\partial n^{m}_{i}}. 
\end{equation}
Now, we compute the sensitivities $\mathbf{s}^{m}$. Note that the reason we call it backpropagation is because the sensitivities are propagated backward through the network via a recurrence relation where the sensitivity at layer $m$ is calculated from the sensitivity at layer $m+1$. This is easier said than done, however, since we need to find a Jacobian matrix of sensitivities to derive the recurrence relation: 
\begin{equation}
\frac{\partial \mathbf{n}^{m+1}}{\partial \mathbf{n}^{m}} \equiv \begin{bmatrix} 
\frac{\partial n^{m+1}_{1}}{\partial n^{m}_{1}} &  \frac{\partial n^{m+1}_{1}}{\partial n^{m}_{2}} & \dots &  \frac{\partial n^{m+1}_{1}}{\partial n^{m}_{S^{m}}} \\
\frac{\partial n^{m+1}_{2}}{\partial n^{m}_{1}} &  \frac{\partial n^{m+1}_{2}}{\partial n^{m}_{2}} & \dots & \frac{\partial n^{m+1}_{2}}{\partial n^{m}_{S^{m}}} \\
\vdots & \vdots &  & \vdots \\
\frac{\partial n^{m+1}_{S^{m+1}}}{\partial n^{m}_{1}} &  \frac{\partial n^{m+1}_{S^{m+1}}}{\partial n^{m}_{2}} & \dots & \frac{\partial n^{m+1}_{S^{m+1}}}{\partial n^{m}_{S^{m}}}
\end{bmatrix}
\end{equation}
To find an expression to represent this Jacobian matrix, we arbitrarily select the $i, j$ element of the matrix in accordance with Hagan's derivation: 
\begin{equation}
\begin{split}
\frac{\partial n^{m+1}_{i}}{\partial \mathbf{n}^{m}_{j}} &= \frac{\partial \bigg(\sum_{l=1}^{S^{m}} w_{i, l}^{m+1}a_{l}^{m} + b_{i}^{m+1}\bigg)}{\partial n_{j}^{m}} = w_{i, j}^{m+1} \frac{\partial a_{j}^{m}}{\partial n_{j}^{m}} \\
&= w_{i, j}^{m+1} \frac{\partial f^{m}(n_{j}^{m}}{\partial n_{j}^{m}} = w_{i, j}^{m+1} \dot{f}^{m}(n_{j}^{m}) ,  
\end{split}
\end{equation}
where 
\begin{equation}
\dot{f}^{m}(n_{j}^{m}) = \frac{\partial f^{m}(n_{j}^{m})}{\partial n_{j}^{m}}.
\end{equation}
Thus, we can write the Jacobian from before as 
\begin{equation}
\frac{\partial \mathbf{n}^{m+1}}{\partial \mathbf{n}^{m}} = \mathbf{W}^{m+1} \dot{\mathbf{F}}^{m}(\mathbf{n}^{m}), 
\end{equation}
where 
\begin{equation}
\dot{\mathbf{F}}^{m}(\mathbf{n}^{m}) = \begin{bmatrix} \dot{f}^{m}(n_{1}^{m}) & 0 & \ldots & 0 \\ 
0 & \dot{f}^{m}(n_{2}^{m}) & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\ 
0 & 0 & \ldots & \dot{f}^{m}(n_{S^{m}}^{m}) \end{bmatrix}.
\end{equation}
Now the recurrence relation for the hidden layer sensitivities can be written as 
\begin{equation}
\begin{split}
\mathbf{s}^{m} & = \dot{\mathbf{F}}^{m}(\mathbf{n}^{m})(\mathbf{W}^{m+1})^{T}\frac{\partial \hat{F}}{\partial \mathbf{n}^{m+1}} \\
& = \dot{\mathbf{F}}^{m}(\mathbf{n}^{m})(\mathbf{W}^{m+1})^{T}\mathbf{s}^{m+1},
\end{split}
\end{equation}
where $\hat{F}$ denotes the performance index as it is an approximation of the network error \cite{hagan2014}. But, we know the network architecture and performance index used, so we can refine the general equation. The transfer function in the hidden layer is the tangent-sigmoid function and we find the derivative as follows: 
\begin{equation}
\begin{split}
\dot{f}^{1} & = \frac{d(\tanh(n))}{dn} = \frac{d}{dn}\bigg(\frac{\sinh(n)}{\cosh(n)}\bigg) \\
 & = \frac{\cosh^{2}(n) - \sinh^{2}(n)}{\cosh^{2}(n)} = 1 - \tanh^{2}(n). 
\end{split}
\end{equation}
Therefore, 
\begin{equation}
\dot{\mathbf{F}}^{1}(\mathbf{n}^{1}) = \begin{bmatrix} 1 - \tanh^{2}(n_{1}^{1}) & 0 & \ldots & 0 \\ 
0 & 1 - \tanh^{2}(n_{2}^{1}) & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\ 
0 & 0 & \ldots & 1 - \tanh^{2}(n_{10}^{1}) \end{bmatrix}.
\end{equation}
and the first layer sensitivity is 
\begin{equation}
\mathbf{s}^{1} = \dot{\mathbf{F}}^{1}(\mathbf{n}^{1})(\mathbf{W}^{2})^{T}\mathbf{s}^{2}.
\end{equation}
The only thing left to do is calculate the sensitivity for the output layer, $\mathbf{s}^{2}$. Calculating the sensitivity for the output layer is a little more difficult as the derivative of the performance index with respect to the net input of the output layer is indirectly a function of the net input of the hidden layer. We can derive the sensitivity for the output layer as follows: 
\begin{equation}
\begin{split}
\mathbf{s}^{2} & = \frac{\partial \hat{F}}{\partial \mathbf{n}_{i}^{2}} = - \sum_{i = 1}^{2} \frac{\partial \mathbf{t}_{j} \ln \mathbf{a}_{j}}{\partial \mathbf{n}_{i}} \\
& = - \sum_{j = 1}^{2} \mathbf{t}_{j} \frac{\partial \ln \mathbf{a}_{j}}{\partial \mathbf{n}_{i}} = - \sum_{j = 1}^{2} \mathbf{t}_{j} \frac{1}{\mathbf{a}_{j}} \frac{\partial \mathbf{a}_{j}}{\partial \mathbf{n}_{i}}
\end{split}
\end{equation}
where $i$ denotes the $i$-th neuron and 
\begin{equation}
\frac{\partial a_{i}}{\partial n_{i}} = \dot{\mathbf{F}}^{2}.
\end{equation}
Now we find the derivative of the \emph{softmax} transfer function to be used in the sensitivity calculation: 
\begin{equation}
\begin{split}
\text{if } i = j \text{: } \frac{\partial a_{i}}{\partial n_{i}} & = \frac{\partial \textbf{softmax}(n_{i})}{\partial n_{i}} = \frac{e^{n_{i}}}{\sum_{n = 1}^{2} e^{n_{i}}} \bigg(\frac{\sum_{n = 1}^{2} e^{n_{i}} - e^{n_{i}}}{\sum_{n = 1}^{2} e^{n_{i}}}\bigg) \\ 
& = \frac{e^{n_{i}}}{\sum_{n = 1}^{2} e^{n_{i}}}\bigg(1 - \frac{e^{n_{i}}}{\sum_{n = 1}^{2} e^{n_{i}}}\bigg) = a_{i}(1 - a_{i}) \\ \\
\text{if } i \neq j \text{: } \frac{\partial a_{i}}{\partial n_{j}} & = \frac{\partial \textbf{softmax}(n_{i})}{\partial n_{j}} = - \frac{e^{n_{i}}}{\sum_{n = 1}^{2} e^{n_{i}}} \frac{e^{n_{j}}}{\sum_{n = 1}^{2} e^{n_{j}}} = -a_{i}a_{j}.
\end{split}
\end{equation}
Using this result to finish the derivation of $\mathbf{s}^{2}$ from (21), we have: 
\begin{equation}
\begin{split}
s_{i}^{2} &= - \sum_{j = 1}^{2} \mathbf{t}_{j} \frac{1}{\mathbf{a}_{j}} \frac{\partial \mathbf{a}_{j}}{\partial \mathbf{n}_{i}} = - \frac{\mathbf{t}_{i}}{\mathbf{a}_{i}} \frac{\partial \mathbf{a}_{i}}{\partial \mathbf{n}_{i}} - \sum_{j \neq i}^{2} \frac{\mathbf{t}_{j}}{\mathbf{a}_{j}} \frac{\partial \mathbf{a}_{j}}{\partial \mathbf{n}_{i}} \\
& = - \frac{\mathbf{t}_{i}}{\mathbf{a}_{i}} \mathbf{a}_{i}(1 - \mathbf{a}_{i}) - \sum_{j \neq i}^{2} \frac{\mathbf{t}_{j}}{\mathbf{a}_{j}}(-a_{i}a_{j}) \\
& = - \mathbf{t}_{i} + \mathbf{t}_{i}\mathbf{a}_{i} + \sum_{j \neq i}^{2} \mathbf{t}_{j}\mathbf{a}_{i} = - \mathbf{t}_{i} + \sum_{j = 1}^{2} \mathbf{t}_{j}\mathbf{a}_{i} \\
& = - \mathbf{t}_{i} + \mathbf{a}_{i} \sum_{j = 1}^{2} \mathbf{t}_{j} = \mathbf{a}_{i}  - \mathbf{t}_{i}.
\end{split}
\end{equation}
Now we have the starting sensitivity: 
\begin{equation}
\mathbf{s}^{2} = \mathbf{a} - \mathbf{t}
\end{equation}
and have everything we need to backpropagate the sensitivities for our network. 

\subsubsection{Update Rule}
\label{sec4.2.4}
Lastly, we need to update the weights and biases via an update rule. For the replication in Python, we use the approximate steepest descent rule to update weights and biases: 
\begin{equation}
\mathbf{W}^{m}(k+1) = \mathbf{W}^{m}(k) - \alpha \mathbf{s}^{m}(\mathbf{a}^{m-1})^{T}, 
\end{equation}
\begin{equation}
\mathbf{b}^{m}(k+1) = \mathbf{b}^{m}(k) - \alpha \mathbf{s}^{m}. 
\end{equation}

\section{Experimental Setup}
\label{sec5}
This section will document how the data are used to train and test the network, implementation of the model in Python, and performance metrics used to evaluate the final model. 

\subsection{Data Preprocessing}
\label{sec5.1}
The data were almost ready to be used when we downloaded it as there were no missing values. The only thing that needed to be changed from the original dataset was the target values. Originally, these values were coded on a one-to-seven scale where values of one-to-four denoted a type of window glass and values of five-to-seven denoted non-window glass. When coding the targets for pattern recognition problems, it is best practice to have as many dimensions in the targets as there are classes. This ensures only one neuron is active at a time and generally produces better results. Further, we coded the targets as $\begin{bmatrix} 1 \\ 0 \end{bmatrix}$ if the glass sample comes from a window and $\begin{bmatrix} 0 \\ 1 \end{bmatrix}$ it it does not for this reason. We briefly mention this in Section \ref{sec3.2} during our discussion of the targets. The code used to import the data and code the targets can be found in Appendix A. 

After collecting the data, we normalize it based on the network architecture we have chosen. Since we use a tangent-sigmoid transfer function in the hidden layer, it is very important that we normalize the inputs between negative one and one since this is the range of the transfer function. When the net input is large (or small), the function becomes saturated. We do not want this to happen at the beginning of the training process because the gradient will be close to zero. Since the first layer's net input is the product of the weight and input plus the bias, large inputs can result in large net inputs if the weights are not small enough scale the inputs down. Thus, it is important to normalize the inputs before we feed them to the network. In order to normalize the inputs to values between negative one and one, we use the following equation: 
\begin{equation}
\mathbf{p}^{n} = 2(\mathbf{p}-\mathbf{p}^{min}) ./ (\mathbf{p}^{max}-\mathbf{p}^{min}) - 1,
\end{equation}
where $\mathbf{p}^{min}$ is the vector containing the minimum values of each element of the input vectors in the dataset, $\mathbf{p}^{max}$ contains the maximum values, ./ represents the element-wise division of the vectors, and $\mathbf{p}^{n}$ is the normalized input vector \cite{hagan2014}. 

Next, we divide the data up into three different sets---training, validation, and testing. We chose to use 70\% of the data for training, 15\% of the data for validation, and 15\% of the data for testing. This is fairly standard practice and follows the literature. To ensure that each of these sets is representative of the full dataset, we split the the data up into two subsets---input-target pairs for window samples and input-target pairs for non-windows. 

Then, we split each of these subsets up into training, validation, and testing sets using the 70:15:15 ratio. After splitting the subsets up into training, validation, and testing, we combine each split-up subset accordingly---i.e. window-training subset with non-window-training subset. This gives us the highest chance of having our subsets be representative of the full dataset and that the subsets cover the same region of the input space. Once the subsets have been combined into complete training, validation, and testing sets, we reshuffle the input-target pairs so that they are randomly-ordered again. The subroutine used to normalize the inputs and split the data can be found in Appendix B. 

\subsection{Implementation of the Network}
\label{sec5.2}
To implement the network, we created a Python class, \verb|NeuronLayer|, to represent layers of a neural network. The \verb|NeuronLayer| class takes the dimension of the inputs, number of neurons in the layer, transfer function, and gradient of the transfer function as arguments. Moreover, there are several functions defined to help train and implement the network in Python. The subroutine used to create the \verb|NeuronLayer| class can be found in Appendix C. 

We begin by instantiating layers from the created class to mimic the network architecture we designed. We arbitrarily chose a learning rate, $\alpha$, of 0.01. If the learning rate converges too slowly, then we will increase the learning rate accordingly. Then, we randomly initialize the weights and biases on the interval $[-0.5, 0.5]$ and pass these on to the corresponding network layers with the \verb|setWeightBias| method. Weights and biases are usually initialized on this interval when we have normalized our inputs between negative one and one. Lastly, we initialize empty vectors to hold the error from each training iteration. These steps can be found in Appendix D. 

\subsection{Training the Network}
We chose to train the network over 100 epochs where one epoch represents a single batch implementation of approximate steepest descent. The training process begins by propagating each input in the training set forward by calling the \verb|FP| method of the \verb|NeuronLayer| class. Then, the cross-entropy is calculated using the equation found in (8). This error is then used to calculate a sensitivity and then propagated backward through the model. Once this process is done for each input in the dataset, we approximate the cross-entropy loss of the epoch by taking the mean of the cross-entropy of the given epoch. Then, we use this information in addition to the learning rate to update the weights and biases by (26) and (27), respectively. 

In attempts to speed up the iterative process used to train the model, we perform the validation simultaneously.  We use the same process to propagate inputs forward and compute the cross-entropy loss, but instead of updating the weights and biases based on the validation set, we use the weights and biases from the training set. This allows us to see if the network is training properly since the validation set is information our model has not been trained on. If the error keeps decreasing in the validation set as the number of iterations increases, this means our network is training properly. To ensure we do not overfit the data with our model, we implement an early stopping technique. We decided to have our network stop training if the validation error increased for five \emph{consecutive} epochs. If this happens to be the case, the script for the model will print to the console that training was stopped early and at what iteration it stopped. The code used to implement this can be found in Appendix E or in the include file \verb|NeuronLayer.py|.  

\subsection{Testing the Network}
Testing the network is a simple process since the network is trained and we just need to feed the inputs in to the network. Since the transfer function in the output layer is the \emph{softmax} function, the network output is the class probabilities between 0 and 1. Once the class probabilities are predicted, we need to use these to convert the probabilities to class predictions. To do this, we use 0.5 as the cutoff, so if one neuron receives a higher probability than the other, that one is chosen to be the positive output from the network and the other is assigned to be null. Then, we compare the predictions to the target which tells us the true class. Using this information, we can calculate classification error, accuracy, true and false positive rates, and construct a confusion matrix. We use all of these as performance metrics in discuss them in the next section. The code used to test the network can be found in Appendix F.

\section{Results}
\label{sec6}



\section{Conclusion}
\label{sec7}


\pagebreak

\printbibliography[heading=bibintoc, title={References}]

\pagebreak

\appendix

\section{Importing the Data \& Coding the Targets}

The Python code used to import the data and code the targets is listed below. First the data are imported with a standard \verb|read_csv| statement from the \verb|pandas| library. Then, the targets are coded by the process stated in Section 3.2. The full subroutine can be found in the included file \verb|DataPrep.py|.

\begin{lstlisting}[style = Python]
def GlassImport():
    # import dataset
    cols = ['id', 'RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe', 'type']
    glass = pd.read_csv('glass.txt', names=cols)
    # create window target column (1 for true; 0 for false)
    glass.ix[glass.ix[:, 'type'] <= 4, 'window'] = 1
    glass.ix[glass.ix[:, 'type'] > 4, 'window'] = 0
    # create non-window target column (1 for true; 0 for false)
    glass.ix[glass.ix[:, 'type'] > 4, 'non_window'] = 1
    glass.ix[glass.ix[:, 'type'] <= 4, 'non_window'] = 0
    # drop id and class columns
    glass.drop(['id', 'type'], inplace=True, axis=1)
    return glass
\end{lstlisting} 
\pagebreak

\section{Data Normalization \& Division}
The following code documents the process used to normalize the inputs and then divide the data up into training, validation, and testing sets. The full subroutine can be found in the included file \verb|Split.py|.

\begin{lstlisting}[style = Python]
def split(dataset, num_inputs, t, v):
    # normalize dataset into range between -1 and 1
    norm = dataset.ix[:, slice(0,num_inputs)].apply(lambda x: -1 + 2 * (x - x.min()) / (x.max() - x.min()), axis=0)
    dataset = pd.concat([norm, dataset.ix[:,'window':]], axis=1)
    # split dataset by target class
    dataset0 = dataset.ix[dataset.ix[:, 'window'] == 0, :]
    dataset1 = dataset.ix[dataset.ix[:, 'window'] == 1, :]
    # build datasets by target class
    train0, validate0, test0 = np.split(dataset0.sample(frac=1),
                                        [int(t * len(dataset0)), int((1 - v) * len(dataset0))])
    train1, validate1, test1 = np.split(dataset1.sample(frac=1),
                                        [int(t * len(dataset1)), int((1 - v) * len(dataset1))])
    # re-combine by target class
    train = pd.concat([train0, train1])
    validate = pd.concat([validate0, validate1])
    test = pd.concat([test0, test1])
    # shuffle order and reset index numbers
    train = train.sample(frac=1)
    train.reset_index(inplace=True)
    validate = validate.sample(frac=1)
    validate.reset_index(inplace=True)
    test = test.sample(frac=1)
    test.reset_index(inplace=True)
    return train.iloc[:, 1:], validate.iloc[:, 1:], test.iloc[:, 1:]
\end{lstlisting} 
\pagebreak

\section{Python Class for Network Layers}
The following code shows the creation of the \verb|NeuronLayer| class. The full subroutine can be found in the included file \verb|NLObjects.py|.

\begin{lstlisting}[style = Python]
class NeuronLayer:
    # define attributes
    def __init__(self, r, s, f, j):
        self.r = r # Number of inputs
        self.s = s # number of neurons
        self.f = f # Transfer function
        self.j = j # derivatve transfer function

    def setWeightBias(self, w, b):
        self.w = w  # set weights
        self.b = b # set biases
        self.r = w.shape[1]
        self.s = w.shape[0]
        return

    def FP(self, p):
        self.p = p
        self.n = self.w * self.p + self.b
        self.a = self.f(self.n)
        return

    def update(self, learning_rate, sensitivity):
        self.w = self.w - learning_rate * sensitivity * self.p.T
        self.b = self.b - learning_rate * sensitivity
        return
\end{lstlisting} 
\pagebreak

\section{Initializing the Network Architecture in Python}
This code shows the process used to instantiate layers and initialize the parameters of the network. 

\begin{lstlisting}[style = Python]
# --------------------- specify network architecture ---------------------
num_neurons1 = 10  # layer 1
num_neurons2 = 2   # layer 2
alpha = 0.01       # learning rate
epoch = 100        # number of iterations
nlayer1 = nl.NeuronLayer(num_neurons1, num_inputs, nl.tansig, nl.j_tansig)      # instantiate layer 1
nlayer2 = nl.NeuronLayer(num_neurons2, num_neurons1, nl.softmax, nl.j_softmax)  # instantiate layer 2
np.random.seed(0)
# randomly initialize weight and bias on the interval [-0.5, 0.5]
W1 = np.matrix(np.random.rand(num_neurons1, num_inputs) - 0.5)
b1 = np.matrix(np.random.rand(num_neurons1, 1) - 0.5)
W2 = np.matrix(np.random.rand(num_neurons2, num_neurons1) - 0.5)
b2 = np.matrix(np.random.rand(num_neurons2, 1) - 0.5)
# initialize cross entropy loss (training & validation)
ce_t = []
ce_v = []
global s1_all, s2_all
# pass on randomly initialized weights and biases to the network
nlayer1.setWeightBias(w=W1, b=b1)
nlayer2.setWeightBias(w=W2, b=b2)
\end{lstlisting} 
\pagebreak

\section{Early Stopping}
The code below shows the early stopping condition used to prevent the model from overfitting. 

\begin{lstlisting}[style = Python]
if j == 0:
	val_fail = []
elif ce_v[j] > ce_v[j-1]:
	val_fail.append(1)
	if len(val_fail) == 5:
		print 'Validation error has increased for 5 consecutive epochs. Early stopping at epoch {}'.format(j)
		break
else:
	val_fail = []
\end{lstlisting} 
\pagebreak

\section{Testing the Network}
The code below shows the code used to test the network and calculate the performance metrics. 

\begin{lstlisting}[style = Python]
# --------------------- confusion matrix ---------------------
actual = pd.Series(test.iloc[:, -2], name='Actual')  # actual values (targets)
input = np.matrix(test.iloc[:, slice(0, num_inputs)]).transpose()  # network inputs, p
nlayer1.FP(input)  # layer 1 net-input
nlayer2.FP(nlayer1.a)  # layer 2 net-input
predict = nl.classify(nlayer2.a)  # predicted values from network
predict = np.array(predict).flatten()
predict = pd.Series(predict, name='Predicted')
confusion = pd.crosstab(actual, predict, margins=False)  # create confusion matrix
confusion = confusion.astype(float)  # convert values to floats
print confusion  # output confusion matrix to the console
# --------------------- accuracy metrics ---------------------
ERR = (confusion.loc[0, 1] + confusion.loc[1, 0]) / len(predict)
ACC = 1 - ERR
FPR = confusion.iloc[0, 1] / (confusion.iloc[0, 1] + confusion.iloc[0, 0])
TPR = confusion.iloc[1, 1] / (confusion.iloc[1, 0] + confusion.iloc[1, 1])
\end{lstlisting} 

\end{document}




