\documentclass[12pt,halfline,a4paper]{ouparticle}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\usepackage[nottoc]{tocbibind}

\addbibresource{paper.bib}

\begin{document}

\title{Using a Two-Layer Neural Network and Physicochemical Properties to Classify Glass for Forensic Analysis}

\author{%
\name{Michael Arango}
\email{mikearango@gwu.edu}
	\and
\name{Mark Barna}
\email{mark.barna@gmail.com}
	\and
\name{Paul Brewster}
\email{pfbrewster@gmail.com}
}

%\abstract{abstract text goes here...}
\date{\today}
\maketitle
\pagebreak
\tableofcontents
\pagebreak

\section{Project Proposal}
\label{sec1}
In this study, we will use the physicochemical properties of glass to determine whether or not a given glass sample was taken from a window. This is a fundamental problem in forensic analysis as it is highly unlikely that glass fragments will be found on people unless they have been present at the time glass breaks. Glass analysis is of vital importance in forensic science as it allows us to test if the glass fragment found on a person is the same as the glass at a crime scene. Since glass is made up of several raw materials and certain elements impart specific properties, we can find out a lot about the glass if we analyze the chemical composition.

The dataset we chose for analysis was made available for download from the UCI Machine Learning Repository and was created by the USA Forensic Science Service. There are 214 observations of 9 different features along with 214 targets that specify whether the glass sample came from a window or not. While we would like more data to train a neural network, we believe the dataset is large enough for our purposes. It is difficult to know before we train a neural network if we have enough data, but the amount of data required is directly related to the complexity of the underlying decision boundary we are trying to implement. We won't 
know how complex the decision boundary we are trying to approximate is until we train the network, but we feel confident using the dataset as many others have used the dataset and found robust results. Several other papers in the literature use much more complex methods than we will employ and have not found the size of the data to be an issue. 

We have chosen a two-layer perceptron network with tangent-sigmoid transfer functions in the hidden layer and \emph{softmax} transfer functions in the output layer. This is a fairly standard network for pattern recognition. Moreover, we will use the \emph{Scaled Conjugate Gradient (SGD)} algorithm to train the network as it is good for pattern recognition problems in which the output layer uses a non-linear transfer function. Since we do not expect the training error to converge to zero, we implement early stopping criteria to prevent overfitting. Lastly, we use \emph{cross-entropy} as our performance index since our targets take on discrete values and it is the optimal performance index for pattern recognition networks that use the \emph{softmax} transfer function in the output layer. 

Two different frameworks will be used to implement the neural network. First, we will use the Neural Network Toolbox, specifically the Neural Network Pattern Recognition Tool (\verb|nprtool|) train, validate, and test our network. We use this framework to start with a simple graphical user interface to quickly ensure our specified network architecture is appropriate and to get baseline performance statistics. Then, we will replicate the analysis in Python to gain practical experience building network architectures in a scripting language. Note that since the goal is practical experience, we will not be leveraging the power of the \emph{scikit-learn} package (\verb|sklearn|) for this exercise. 

Several reference materials will be consulted to obtain sufficient background knowledge of the subject at hand. First, we plan on doing a thorough review of the forensic chemistry and geology literature to understand the reasons for using physicochemical properties to classify glass. Then, papers on glass analysis will be examined to supplement background knowledge with experiential knowledge. 

Considering our problem is one of pattern recognition, a confusion matrix will be used to assess the accuracy of our model and the \emph{false postive} (Type I error) and \emph{false negative} (Type II error) rates. Further, the \emph{Receiver Operating Characteristic (ROC) curve} will be used to compare the true positive rate to the false positive rate. This will help us gain additional knowledge of the predictive power of our network. 

We plan to finish our research and submit it by Wednesday, June 28, 2017. 

\section{Introduction}
\label{sec2}
An overview of the project and an outline of the report (I like to write intro after I finish a project).

\subsection{Literature Review}
\label{sec3}

\section{Description of the Dataset}
\label{sec4}
The dataset was made available for download from the UCI Machine Learning Repository and was created by the USA Forensic Science Service \cite{murphy1994}. The purpose of this dataset is to use physicochemical properties to classify whether a certain glass fragment comes from a window or not. 

\subsection{Inputs}
The matrix of inputs contains 214 observations of 9 variables and there is no missing data. Of these variables, eight of the nine measure the percent weight that a given elemental oxide makes up of the total glass sample weight. All the eight elements except silicon are classified as metals on the periodic table of elements. Sodium and potassium are alkali metals whereas magnesium, calcium, and barium alkaline earth metals. Aluminum and iron are classified as poor metals and transition metals, respectively. The last variable in the input matrix represents the refractive index which measures the speed of light in a transparent medium and is known as Snell's law. It can be represented formulaically as the ratio of the velocity of light in a vacuum to the velocity of light in the glass itself: $n = \frac{c}{v}$. A more thorough description of target variables is as follows: 
\begin{description}
\item \textbf{Refractive Index:}
measures the ratio of the velocity of light in a vacuum to the velocity of light in the glass itself
\item \textbf{Sodium:}
represents the percent weight in sodium oxide ($\mathrm{Na_{2}O}$)
\item \textbf{Magnesium:}
represents the percent weight in magnesium oxide ($\mathrm{MgO}$)
\item \textbf{Aluminum:}
represents the percent weight in aluminum oxide ($\mathrm{Na_{2}O}$)
\item \textbf{Silicon:}
represents the percent weight in silicon oxide ($\mathrm{Al_{2}O_{3}}$)
\item \textbf{Potassium:}
represents the percent weight in potassium oxide ($\mathrm{K_{2}O}$)
\item \textbf{Calcium:}
represents the percent weight in calcium oxide ($\mathrm{Ca_{2}O}$)
\item \textbf{Barium:}
represents the percent weight in barium oxide ($\mathrm{Ba_{2}O}$)
\item \textbf{Iron:}
represents the percent weight in iron oxide ($\mathrm{Fe_{2}O_{3}}$)
\end{description}

\subsection{Targets}
The matrix of targets has 214 observations, one for each observation in the training set, where a given target is denoted by $\begin{bmatrix} 1 \\ 0 \end{bmatrix}$ if the glass sample comes from a window and $\begin{bmatrix} 0 \\ 1 \end{bmatrix}$ otherwise. Note that the targets are two-dimensional instead of the more common one-dimensional binary encoding. This two-dimensional encoding allows us to have only one neuron firing at a time and tends to result in marginally better performance. 


\section{Description of the Network Architecture and Training Algorithm}

\subsection{Network Architecture}
\subsection{Training Algorith}

\section{Experimental Setup}

\subsection{Data Preprocessing}
\subsection{Implementation of the Network}
\subsection{Performance Index}

\section{Results}

\section{Conclusion}


\subsection{Figures and tables}
\label{sec3.9}

The following is an example of typesetting a table.

\begin{verbatim}
\begin{table}
\caption{Table caption text.}
\label{key}
The table matter goes here.
\end{table}
\end{verbatim}

As always with \LaTeX, the \verb+\label+ must be after the
\verb+\caption+, and inside the figure or table environment. The reference for
figures and tables inside text can be made using the \verb|\ref{key}| command.

\subsection{Equations}\label{sec3.10}

For example, if you type
\begin{verbatim}
\begin{equation}\label{eq1}
\int^{r_2}_0 F(r,\varphi){\rm d}r\,{\rm d}\varphi = [\sigma r_2/(2\mu_0)]
\int^{\infty}_0\exp(-\lambda|z_j-z_i|)\lambda^{-1}J_1 (\lambda r_2)J_0
(\lambda r_i\,\lambda {\rm d}\lambda)
\end{equation}
\end{verbatim}
then you will get the following output:
\begin{equation}\label{eq1}
\int^{r_2}_0 F(r,\varphi){\rm d}r\,{\rm d}\varphi = [\sigma r_2/(2\mu_0)]\int^{\infty}_0
\exp(-\lambda|z_j-z_i|)\lambda^{-1}J_1 (\lambda r_2)J_0 (\lambda r_i\,\lambda {\rm d}\lambda)
\end{equation}

\subsection{Listings}
\label{sec3.12}

Another frequently displayed structure is a list. The
following is an example of an \emph{itemized} list.
\begin{itemize}
   \item This is the first item of an itemized list.
         Each item in the list is marked with a
         `$\bullet$'.

   \item This is the second item of the list. It
         contains another list nested inside it. The
         inner list is an \emph{enumerated} list.
         \begin{enumerate}
            \item This is the first item of an enumerated
                  list that is nested within the
                  itemized list.

            \item This is the second item of the inner list.
                  \LaTeX\ allows you to nest lists
                  deeper than you really should.
         \end{enumerate}
         This is the rest of the second item of the
         outer list. It is no more interesting than
         any other part of the item.
   \item This is the third item of the list.
\end{itemize}


\pagebreak

\printbibliography[heading=bibintoc, title={References}]

%\addcontentsline{toc}{section}{References}
%\begin{thebibliography}{0}
%\bibitem{bib1}
%Murphy, P.M., Aha, D.W. (1994): {\em UCI Repository of Machine Learning Databases.} 
%[http://www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, CA: University of California, Department of Information and Computer Science.
%\bibitem{bib2}
%Knuth, D.E: {\em The {\TeX}book}. Addison-Wesley, Reading, MA, USA, 1984.
%\bibitem{bib3}
%Lamport, L.: {\em {\LaTeX} -- A Document Preparation System -- User's
%Guide and Reference Manual}. Addison-Wesley, Reading, MA, USA, 1985.
%\bibitem{bib4}
%Smith, I.N., R.S. Johnes, and W.P. Hines: 1992, `Title of the Article',
%\textit{Journal Title in Italics} \textbf{Vol. no. X}, pp. 00--00
%\end{thebibliography}

\pagebreak
\appendix

\section{Perceptron}
\section{Backprop}
\section{Subroutine X}

\end{document}
